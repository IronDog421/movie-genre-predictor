{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "946f301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy.sparse import hstack as sp_hstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b39dd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = Path(\"../dataset_train.csv\")\n",
    "test_dir = Path(\"../dataset_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fee8e580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 8475\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_name</th>\n",
       "      <th>genre</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silent Hill</td>\n",
       "      <td>Horror, Mystery</td>\n",
       "      <td>Rose, a desperate mother takes her adopted dau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Breaking the Waves</td>\n",
       "      <td>Drama, Romance</td>\n",
       "      <td>In a small and conservative Scottish village, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wind Chill</td>\n",
       "      <td>Drama, Horror, Thriller</td>\n",
       "      <td>Two college students share a ride home for the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Godmothered</td>\n",
       "      <td>Family, Fantasy, Comedy</td>\n",
       "      <td>A young and unskilled fairy godmother that ven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Donkey Skin</td>\n",
       "      <td>Fantasy, Comedy, Music, Romance</td>\n",
       "      <td>A fairy godmother helps a princess disguise he...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           movie_name                            genre  \\\n",
       "0         Silent Hill                  Horror, Mystery   \n",
       "1  Breaking the Waves                   Drama, Romance   \n",
       "2          Wind Chill          Drama, Horror, Thriller   \n",
       "3         Godmothered          Family, Fantasy, Comedy   \n",
       "4         Donkey Skin  Fantasy, Comedy, Music, Romance   \n",
       "\n",
       "                                         description  \n",
       "0  Rose, a desperate mother takes her adopted dau...  \n",
       "1  In a small and conservative Scottish village, ...  \n",
       "2  Two college students share a ride home for the...  \n",
       "3  A young and unskilled fairy godmother that ven...  \n",
       "4  A fairy godmother helps a princess disguise he...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(train_dir)\n",
    "print(f\"Dataset size: {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e44a3c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"movie_name\"].fillna(\"\") + \" [SEP] \" + df[\"description\"].fillna(\"\")\n",
    "y_list = df[\"genre\"].apply(lambda s: [g.strip() for g in str(s).split(\",\") if g.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f9c36b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 7627, Validation samples: 848\n",
      "Number of labels: 18\n"
     ]
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(y_list)\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(df[\"text\"], Y, test_size=0.1, random_state=42)\n",
    "print(f\"Training samples: {len(X_tr)}, Validation samples: {len(X_va)}\")\n",
    "print(f\"Number of labels: {len(mlb.classes_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dad1d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word TF-IDF shape: (7627, 29337), Char TF-IDF shape: (7627, 128989)\n"
     ]
    }
   ],
   "source": [
    "tfidf_word = TfidfVectorizer(\n",
    "    ngram_range=(1,3),\n",
    "    min_df=2,\n",
    "    max_features=500_000,\n",
    "    sublinear_tf=True,\n",
    "    stop_words=\"english\",\n",
    "    max_df=0.85,\n",
    "    strip_accents='unicode',\n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "tfidf_char = TfidfVectorizer(\n",
    "    analyzer=\"char_wb\",\n",
    "    ngram_range=(3,6),\n",
    "    min_df=2,\n",
    "    max_features=500_000,\n",
    "    sublinear_tf=True,\n",
    "    max_df=0.85,\n",
    "    strip_accents='unicode'\n",
    ")\n",
    "\n",
    "Xw_tr = tfidf_word.fit_transform(X_tr)\n",
    "Xw_va = tfidf_word.transform(X_va)\n",
    "Xc_tr = tfidf_char.fit_transform(X_tr)\n",
    "Xc_va = tfidf_char.transform(X_va)\n",
    "print(f\"Word TF-IDF shape: {Xw_tr.shape}, Char TF-IDF shape: {Xc_tr.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0631668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined TF-IDF features shape: (7627, 158326)\n"
     ]
    }
   ],
   "source": [
    "XTR_tfidf = sp_hstack([Xw_tr, Xc_tr], format=\"csr\")\n",
    "XVA_tfidf = sp_hstack([Xw_va, Xc_va], format=\"csr\")\n",
    "print(f\"Combined TF-IDF features shape: {XTR_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4d034bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shiyi Cheng yi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Shiyi Cheng yi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Loading sentence transformer model...\n",
      "Loading sentence transformer model...\n",
      "Generating embeddings for training set...\n",
      "Generating embeddings for training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 239/239 [00:57<00:00,  4.18it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 27/27 [00:06<00:00,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (7627, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(\"Loading sentence transformer model...\")\n",
    "st_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Generating embeddings for training set...\")\n",
    "emb_tr = st_model.encode(X_tr.tolist(), show_progress_bar=True, batch_size=32)\n",
    "print(\"Generating embeddings for validation set...\")\n",
    "emb_va = st_model.encode(X_va.tolist(), show_progress_bar=True, batch_size=32)\n",
    "print(f\"Embedding shape: {emb_tr.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d687eb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined features (TF-IDF + Embeddings) shape: (7627, 158710)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "XTR_combined = sp_hstack([XTR_tfidf, csr_matrix(emb_tr)], format=\"csr\")\n",
    "XVA_combined = sp_hstack([XVA_tfidf, csr_matrix(emb_va)], format=\"csr\")\n",
    "print(f\"Combined features (TF-IDF + Embeddings) shape: {XTR_combined.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7fd7a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LogisticRegression with combined features...\n",
      "Training complete!\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "clf_logreg = OneVsRestClassifier(\n",
    "    LogisticRegression(C=8.0, solver=\"saga\", max_iter=4000, class_weight='balanced', random_state=42),\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(\"Training LogisticRegression with combined features...\")\n",
    "clf_logreg.fit(XTR_combined, y_tr)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28f81cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating thresholds for LogReg...\n",
      "LogReg - micro-F1: 0.5966, macro-F1: 0.6031\n",
      "LogReg - micro-F1: 0.5966, macro-F1: 0.6031\n"
     ]
    }
   ],
   "source": [
    "logits_logreg = clf_logreg.decision_function(XVA_combined)\n",
    "ths_logreg = np.zeros(logits_logreg.shape[1])\n",
    "\n",
    "print(\"Calibrating thresholds for LogReg...\")\n",
    "for k in range(logits_logreg.shape[1]):\n",
    "    s = logits_logreg[:, k]\n",
    "    best_f1, best_t = 0.0, 0.0\n",
    "    candidates = np.concatenate([\n",
    "        np.quantile(s, np.linspace(0.01, 0.99, 50)),\n",
    "        [s.mean(), np.median(s), 0.0, -0.5, 0.5],\n",
    "    ])\n",
    "    candidates = np.unique(candidates)\n",
    "    for t in candidates:\n",
    "        preds_k = (s >= t).astype(int)\n",
    "        f1 = f1_score(y_va[:, k], preds_k, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, t\n",
    "    ths_logreg[k] = best_t\n",
    "\n",
    "pred_logreg = (logits_logreg >= ths_logreg).astype(int)\n",
    "print(f\"LogReg - micro-F1: {f1_score(y_va, pred_logreg, average='micro'):.4f}, macro-F1: {f1_score(y_va, pred_logreg, average='macro'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ae7e4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost with embeddings...\n",
      "Training complete!\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "clf_xgb = MultiOutputClassifier(\n",
    "    XGBClassifier(n_estimators=300, max_depth=6, learning_rate=0.1, random_state=42, n_jobs=-1)\n",
    ")\n",
    "print(\"Training XGBoost with embeddings...\")\n",
    "clf_xgb.fit(emb_tr, y_tr)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88c581bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating thresholds for XGBoost...\n",
      "XGBoost - micro-F1: 0.6311, macro-F1: 0.5757\n",
      "XGBoost - micro-F1: 0.6311, macro-F1: 0.5757\n"
     ]
    }
   ],
   "source": [
    "pred_proba_xgb = clf_xgb.predict_proba(emb_va)\n",
    "logits_xgb = np.column_stack([p[:, 1] for p in pred_proba_xgb])\n",
    "ths_xgb = np.zeros(logits_xgb.shape[1])\n",
    "\n",
    "print(\"Calibrating thresholds for XGBoost...\")\n",
    "for k in range(logits_xgb.shape[1]):\n",
    "    s = logits_xgb[:, k]\n",
    "    best_f1, best_t = 0.0, 0.0\n",
    "    candidates = np.concatenate([\n",
    "        np.quantile(s, np.linspace(0.01, 0.99, 50)),\n",
    "        [s.mean(), np.median(s), 0.0, 0.3, 0.5, 0.7],\n",
    "    ])\n",
    "    candidates = np.unique(candidates)\n",
    "    for t in candidates:\n",
    "        preds_k = (s >= t).astype(int)\n",
    "        f1 = f1_score(y_va[:, k], preds_k, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, t\n",
    "    ths_xgb[k] = best_t\n",
    "\n",
    "pred_xgb = (logits_xgb >= ths_xgb).astype(int)\n",
    "print(f\"XGBoost - micro-F1: {f1_score(y_va, pred_xgb, average='micro'):.4f}, macro-F1: {f1_score(y_va, pred_xgb, average='macro'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e43b6c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LinearSVC with TF-IDF features...\n",
      "Training complete!\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf_svc = OneVsRestClassifier(\n",
    "    LinearSVC(C=2.0, max_iter=4000, class_weight='balanced', dual='auto', random_state=42),\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(\"Training LinearSVC with TF-IDF features...\")\n",
    "clf_svc.fit(XTR_tfidf, y_tr)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb1a6555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating thresholds for LinearSVC...\n",
      "LinearSVC - micro-F1: 0.6256, macro-F1: 0.5597\n",
      "LinearSVC - micro-F1: 0.6256, macro-F1: 0.5597\n"
     ]
    }
   ],
   "source": [
    "logits_svc = clf_svc.decision_function(XVA_tfidf)\n",
    "ths_svc = np.zeros(logits_svc.shape[1])\n",
    "\n",
    "print(\"Calibrating thresholds for LinearSVC...\")\n",
    "for k in range(logits_svc.shape[1]):\n",
    "    s = logits_svc[:, k]\n",
    "    best_f1, best_t = 0.0, 0.0\n",
    "    candidates = np.concatenate([\n",
    "        np.quantile(s, np.linspace(0.01, 0.99, 50)),\n",
    "        [s.mean(), np.median(s), 0.0, -0.5, 0.5],\n",
    "    ])\n",
    "    candidates = np.unique(candidates)\n",
    "    for t in candidates:\n",
    "        preds_k = (s >= t).astype(int)\n",
    "        f1 = f1_score(y_va[:, k], preds_k, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, t\n",
    "    ths_svc[k] = best_t\n",
    "\n",
    "pred_svc = (logits_svc >= ths_svc).astype(int)\n",
    "print(f\"LinearSVC - micro-F1: {f1_score(y_va, pred_svc, average='micro'):.4f}, macro-F1: {f1_score(y_va, pred_svc, average='macro'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f72c9947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating thresholds for ensemble...\n",
      "Ensemble - micro-F1: 0.6012, macro-F1: 0.6073\n",
      "Ensemble - micro-F1: 0.6012, macro-F1: 0.6073\n"
     ]
    }
   ],
   "source": [
    "ensemble_logits = 0.5 * logits_logreg + 0.35 * logits_xgb + 0.15 * logits_svc\n",
    "ths_ensemble = np.zeros(ensemble_logits.shape[1])\n",
    "\n",
    "print(\"Calibrating thresholds for ensemble...\")\n",
    "for k in range(ensemble_logits.shape[1]):\n",
    "    s = ensemble_logits[:, k]\n",
    "    best_f1, best_t = 0.0, 0.0\n",
    "    candidates = np.concatenate([\n",
    "        np.quantile(s, np.linspace(0.005, 0.995, 60)),\n",
    "        [s.mean(), np.median(s), 0.0, -1.0, -0.5, 0.5, 1.0],\n",
    "        np.linspace(s.min(), s.max(), 20)\n",
    "    ])\n",
    "    candidates = np.unique(candidates)\n",
    "    for t in candidates:\n",
    "        preds_k = (s >= t).astype(int)\n",
    "        f1 = f1_score(y_va[:, k], preds_k, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, t\n",
    "    ths_ensemble[k] = best_t\n",
    "\n",
    "pred_ensemble = (ensemble_logits >= ths_ensemble).astype(int)\n",
    "print(f\"Ensemble - micro-F1: {f1_score(y_va, pred_ensemble, average='micro'):.4f}, macro-F1: {f1_score(y_va, pred_ensemble, average='macro'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6677caf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PERFORMANCE COMPARISON\n",
      "============================================================\n",
      "1. LogReg (TF-IDF+Embeddings):  micro-F1: 0.5966, macro-F1: 0.6031\n",
      "2. XGBoost (Embeddings):        micro-F1: 0.6311, macro-F1: 0.5757\n",
      "3. LinearSVC (TF-IDF):          micro-F1: 0.6256, macro-F1: 0.5597\n",
      "4. ENSEMBLE (All):              micro-F1: 0.6012, macro-F1: 0.6073\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"1. LogReg (TF-IDF+Embeddings):  micro-F1: {f1_score(y_va, pred_logreg, average='micro'):.4f}, macro-F1: {f1_score(y_va, pred_logreg, average='macro'):.4f}\")\n",
    "print(f\"2. XGBoost (Embeddings):        micro-F1: {f1_score(y_va, pred_xgb, average='micro'):.4f}, macro-F1: {f1_score(y_va, pred_xgb, average='macro'):.4f}\")\n",
    "print(f\"3. LinearSVC (TF-IDF):          micro-F1: {f1_score(y_va, pred_svc, average='micro'):.4f}, macro-F1: {f1_score(y_va, pred_svc, average='macro'):.4f}\")\n",
    "print(f\"4. ENSEMBLE (All):              micro-F1: {f1_score(y_va, pred_ensemble, average='micro'):.4f}, macro-F1: {f1_score(y_va, pred_ensemble, average='macro'):.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59457d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models and artifacts saved successfully!\n"
     ]
    }
   ],
   "source": [
    "joblib.dump(tfidf_word, \"tfidf_word.joblib\")\n",
    "joblib.dump(tfidf_char, \"tfidf_char.joblib\")\n",
    "joblib.dump(st_model, \"sentence_transformer.joblib\")\n",
    "joblib.dump(clf_logreg, \"clf_logreg.joblib\")\n",
    "joblib.dump(clf_xgb, \"clf_xgb.joblib\")\n",
    "joblib.dump(clf_svc, \"clf_svc.joblib\")\n",
    "with open(\"labels.json\", \"w\") as f:\n",
    "    json.dump(mlb.classes_.tolist(), f)\n",
    "np.save(\"thresholds_logreg.npy\", ths_logreg)\n",
    "np.save(\"thresholds_xgb.npy\", ths_xgb)\n",
    "np.save(\"thresholds_svc.npy\", ths_svc)\n",
    "np.save(\"thresholds_ensemble.npy\", ths_ensemble)\n",
    "print(\"All models and artifacts saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49b95501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 30/30 [00:07<00:00,  4.11it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions saved! Generated 942 predictions.\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv(test_dir)\n",
    "df_test[\"text\"] = df_test[\"movie_name\"].fillna(\"\") + \" [SEP] \" + df_test[\"description\"].fillna(\"\")\n",
    "\n",
    "Xw_test = tfidf_word.transform(df_test[\"text\"])\n",
    "Xc_test = tfidf_char.transform(df_test[\"text\"])\n",
    "X_test_tfidf = sp_hstack([Xw_test, Xc_test], format=\"csr\")\n",
    "emb_test = st_model.encode(df_test[\"text\"].tolist(), show_progress_bar=True, batch_size=32)\n",
    "X_test_combined = sp_hstack([X_test_tfidf, csr_matrix(emb_test)], format=\"csr\")\n",
    "\n",
    "logits_logreg_test = clf_logreg.decision_function(X_test_combined)\n",
    "pred_proba_xgb_test = clf_xgb.predict_proba(emb_test)\n",
    "logits_xgb_test = np.column_stack([p[:, 1] for p in pred_proba_xgb_test])\n",
    "logits_svc_test = clf_svc.decision_function(X_test_tfidf)\n",
    "\n",
    "ensemble_logits_test = 0.5 * logits_logreg_test + 0.35 * logits_xgb_test + 0.15 * logits_svc_test\n",
    "pred_test = (ensemble_logits_test >= ths_ensemble).astype(int)\n",
    "\n",
    "pred_labels = [\", \".join([mlb.classes_[j] for j, v in enumerate(row) if v == 1]) for row in pred_test]\n",
    "result_df = pd.DataFrame({\n",
    "    \"movie_name\": df_test[\"movie_name\"],\n",
    "    \"genre\": pred_labels,\n",
    "    \"description\": df_test[\"description\"]\n",
    "})\n",
    "result_df.to_csv(\"dataset_test_preds.csv\", index=False)\n",
    "print(f\"Test predictions saved! Generated {len(result_df)} predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4817946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DistilBERT tokenizer and model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shiyi Cheng yi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Shiyi Cheng yi\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets created: 7627 training, 848 validation\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class MovieGenreDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx]) if hasattr(self.texts, 'iloc') else str(self.texts[idx])\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "print(\"Loading DistilBERT tokenizer and model...\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=len(mlb.classes_),\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "train_dataset = MovieGenreDataset(X_tr, y_tr, tokenizer)\n",
    "val_dataset = MovieGenreDataset(X_va, y_va, tokenizer)\n",
    "print(f\"Datasets created: {len(train_dataset)} training, {len(val_dataset)} validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7682ea0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DistilBERT fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shiyi Cheng yi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1431' max='1431' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1431/1431 1:03:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.283400</td>\n",
       "      <td>0.253061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.217400</td>\n",
       "      <td>0.220534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.160500</td>\n",
       "      <td>0.215089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shiyi Cheng yi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Shiyi Cheng yi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Shiyi Cheng yi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning complete!\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./distilbert_results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "print(\"Starting DistilBERT fine-tuning...\")\n",
    "trainer.train()\n",
    "print(\"Fine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac028e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating thresholds for DistilBERT...\n",
      "DistilBERT - micro-F1: 0.6991, macro-F1: 0.6486\n",
      "DistilBERT - micro-F1: 0.6991, macro-F1: 0.6486\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_inputs = tokenizer(X_va.tolist(), truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "    outputs = model(**val_inputs)\n",
    "    logits_distilbert = torch.sigmoid(outputs.logits).cpu().numpy()\n",
    "\n",
    "ths_distilbert = np.zeros(logits_distilbert.shape[1])\n",
    "\n",
    "print(\"Calibrating thresholds for DistilBERT...\")\n",
    "for k in range(logits_distilbert.shape[1]):\n",
    "    s = logits_distilbert[:, k]\n",
    "    best_f1, best_t = 0.0, 0.0\n",
    "    candidates = np.concatenate([\n",
    "        np.quantile(s, np.linspace(0.01, 0.99, 50)),\n",
    "        [s.mean(), np.median(s), 0.3, 0.5, 0.7],\n",
    "    ])\n",
    "    candidates = np.unique(candidates)\n",
    "    for t in candidates:\n",
    "        preds_k = (s >= t).astype(int)\n",
    "        f1 = f1_score(y_va[:, k], preds_k, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, t\n",
    "    ths_distilbert[k] = best_t\n",
    "\n",
    "pred_distilbert = (logits_distilbert >= ths_distilbert).astype(int)\n",
    "print(f\"DistilBERT - micro-F1: {f1_score(y_va, pred_distilbert, average='micro'):.4f}, macro-F1: {f1_score(y_va, pred_distilbert, average='macro'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da73404a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating thresholds for FINAL ensemble (with DistilBERT)...\n",
      "FINAL ENSEMBLE - micro-F1: 0.6178, macro-F1: 0.6248\n",
      "FINAL ENSEMBLE - micro-F1: 0.6178, macro-F1: 0.6248\n"
     ]
    }
   ],
   "source": [
    "ensemble_final_logits = 0.4 * logits_distilbert + 0.25 * logits_logreg + 0.2 * logits_xgb + 0.15 * logits_svc\n",
    "ths_ensemble_final = np.zeros(ensemble_final_logits.shape[1])\n",
    "\n",
    "print(\"Calibrating thresholds for FINAL ensemble (with DistilBERT)...\")\n",
    "for k in range(ensemble_final_logits.shape[1]):\n",
    "    s = ensemble_final_logits[:, k]\n",
    "    best_f1, best_t = 0.0, 0.0\n",
    "    candidates = np.concatenate([\n",
    "        np.quantile(s, np.linspace(0.005, 0.995, 70)),\n",
    "        [s.mean(), np.median(s), 0.0, 0.3, 0.5, 0.7],\n",
    "        np.linspace(s.min(), s.max(), 25)\n",
    "    ])\n",
    "    candidates = np.unique(candidates)\n",
    "    for t in candidates:\n",
    "        preds_k = (s >= t).astype(int)\n",
    "        f1 = f1_score(y_va[:, k], preds_k, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, t\n",
    "    ths_ensemble_final[k] = best_t\n",
    "\n",
    "pred_ensemble_final = (ensemble_final_logits >= ths_ensemble_final).astype(int)\n",
    "print(f\"FINAL ENSEMBLE - micro-F1: {f1_score(y_va, pred_ensemble_final, average='micro'):.4f}, macro-F1: {f1_score(y_va, pred_ensemble_final, average='macro'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "730b6f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FINAL PERFORMANCE COMPARISON - ALL MODELS\n",
      "======================================================================\n",
      "1. LogReg (TF-IDF+Embed):       micro-F1: 0.5966, macro-F1: 0.6031\n",
      "2. XGBoost (Embeddings):        micro-F1: 0.6311, macro-F1: 0.5757\n",
      "3. LinearSVC (TF-IDF):          micro-F1: 0.6256, macro-F1: 0.5597\n",
      "4. DistilBERT (Fine-tuned):     micro-F1: 0.6991, macro-F1: 0.6486\n",
      "5. Ensemble (no DistilBERT):    micro-F1: 0.6012, macro-F1: 0.6073\n",
      "6. FINAL ENSEMBLE (All 4):      micro-F1: 0.6178, macro-F1: 0.6248\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FINAL PERFORMANCE COMPARISON - ALL MODELS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"1. LogReg (TF-IDF+Embed):       micro-F1: {f1_score(y_va, pred_logreg, average='micro'):.4f}, macro-F1: {f1_score(y_va, pred_logreg, average='macro'):.4f}\")\n",
    "print(f\"2. XGBoost (Embeddings):        micro-F1: {f1_score(y_va, pred_xgb, average='micro'):.4f}, macro-F1: {f1_score(y_va, pred_xgb, average='macro'):.4f}\")\n",
    "print(f\"3. LinearSVC (TF-IDF):          micro-F1: {f1_score(y_va, pred_svc, average='micro'):.4f}, macro-F1: {f1_score(y_va, pred_svc, average='macro'):.4f}\")\n",
    "print(f\"4. DistilBERT (Fine-tuned):     micro-F1: {f1_score(y_va, pred_distilbert, average='micro'):.4f}, macro-F1: {f1_score(y_va, pred_distilbert, average='macro'):.4f}\")\n",
    "print(f\"5. Ensemble (no DistilBERT):    micro-F1: {f1_score(y_va, pred_ensemble, average='micro'):.4f}, macro-F1: {f1_score(y_va, pred_ensemble, average='macro'):.4f}\")\n",
    "print(f\"6. FINAL ENSEMBLE (All 4):      micro-F1: {f1_score(y_va, pred_ensemble_final, average='micro'):.4f}, macro-F1: {f1_score(y_va, pred_ensemble_final, average='macro'):.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0732f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models and artifacts saved successfully!\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"./distilbert_model\")\n",
    "tokenizer.save_pretrained(\"./distilbert_model\")\n",
    "joblib.dump(tfidf_word, \"tfidf_word.joblib\")\n",
    "joblib.dump(tfidf_char, \"tfidf_char.joblib\")\n",
    "joblib.dump(st_model, \"sentence_transformer.joblib\")\n",
    "joblib.dump(clf_logreg, \"clf_logreg.joblib\")\n",
    "joblib.dump(clf_xgb, \"clf_xgb.joblib\")\n",
    "joblib.dump(clf_svc, \"clf_svc.joblib\")\n",
    "with open(\"labels.json\", \"w\") as f:\n",
    "    json.dump(mlb.classes_.tolist(), f)\n",
    "np.save(\"thresholds_logreg.npy\", ths_logreg)\n",
    "np.save(\"thresholds_xgb.npy\", ths_xgb)\n",
    "np.save(\"thresholds_svc.npy\", ths_svc)\n",
    "np.save(\"thresholds_distilbert.npy\", ths_distilbert)\n",
    "np.save(\"thresholds_ensemble.npy\", ths_ensemble)\n",
    "np.save(\"thresholds_ensemble_final.npy\", ths_ensemble_final)\n",
    "print(\"All models and artifacts saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c1724aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 30/30 [00:09<00:00,  3.06it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL test predictions saved! Generated 942 predictions.\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv(test_dir)\n",
    "df_test[\"text\"] = df_test[\"movie_name\"].fillna(\"\") + \" [SEP] \" + df_test[\"description\"].fillna(\"\")\n",
    "\n",
    "Xw_test = tfidf_word.transform(df_test[\"text\"])\n",
    "Xc_test = tfidf_char.transform(df_test[\"text\"])\n",
    "X_test_tfidf = sp_hstack([Xw_test, Xc_test], format=\"csr\")\n",
    "emb_test = st_model.encode(df_test[\"text\"].tolist(), show_progress_bar=True, batch_size=32)\n",
    "X_test_combined = sp_hstack([X_test_tfidf, csr_matrix(emb_test)], format=\"csr\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_inputs = tokenizer(df_test[\"text\"].tolist(), truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "    outputs = model(**test_inputs)\n",
    "    logits_distilbert_test = torch.sigmoid(outputs.logits).cpu().numpy()\n",
    "\n",
    "logits_logreg_test = clf_logreg.decision_function(X_test_combined)\n",
    "pred_proba_xgb_test = clf_xgb.predict_proba(emb_test)\n",
    "logits_xgb_test = np.column_stack([p[:, 1] for p in pred_proba_xgb_test])\n",
    "logits_svc_test = clf_svc.decision_function(X_test_tfidf)\n",
    "\n",
    "ensemble_final_logits_test = 0.4 * logits_distilbert_test + 0.25 * logits_logreg_test + 0.2 * logits_xgb_test + 0.15 * logits_svc_test\n",
    "pred_test = (ensemble_final_logits_test >= ths_ensemble_final).astype(int)\n",
    "\n",
    "pred_labels = [\", \".join([mlb.classes_[j] for j, v in enumerate(row) if v == 1]) for row in pred_test]\n",
    "result_df = pd.DataFrame({\n",
    "    \"movie_name\": df_test[\"movie_name\"],\n",
    "    \"genre\": pred_labels,\n",
    "    \"description\": df_test[\"description\"]\n",
    "})\n",
    "result_df.to_csv(\"dataset_test_preds_final.csv\", index=False)\n",
    "print(f\"FINAL test predictions saved! Generated {len(result_df)} predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf93909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeab351",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
