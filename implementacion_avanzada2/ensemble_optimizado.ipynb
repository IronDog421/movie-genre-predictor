{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f02983d",
   "metadata": {},
   "source": [
    "# Ensemble Optimizado para Predicción de Géneros\n",
    "\n",
    "Este notebook implementa un ensemble avanzado con:\n",
    "- Data augmentation (back-translation)\n",
    "- DeBERTa-v3 (modelo top del leaderboard)\n",
    "- Optimización de pesos con Optuna\n",
    "- Test-time augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b770b8d4",
   "metadata": {},
   "source": [
    "## 1. Imports y Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "973ad285",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shiyi Cheng yi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Shiyi Cheng yi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy.sparse import hstack as sp_hstack, csr_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import (\n",
    "    DistilBertTokenizer, DistilBertForSequenceClassification,\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments,\n",
    "    MarianMTModel, MarianTokenizer\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7131b8",
   "metadata": {},
   "source": [
    "## 2. Carga y Preparación de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb3a087d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 8475\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_name</th>\n",
       "      <th>genre</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silent Hill</td>\n",
       "      <td>Horror, Mystery</td>\n",
       "      <td>Rose, a desperate mother takes her adopted dau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Breaking the Waves</td>\n",
       "      <td>Drama, Romance</td>\n",
       "      <td>In a small and conservative Scottish village, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wind Chill</td>\n",
       "      <td>Drama, Horror, Thriller</td>\n",
       "      <td>Two college students share a ride home for the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Godmothered</td>\n",
       "      <td>Family, Fantasy, Comedy</td>\n",
       "      <td>A young and unskilled fairy godmother that ven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Donkey Skin</td>\n",
       "      <td>Fantasy, Comedy, Music, Romance</td>\n",
       "      <td>A fairy godmother helps a princess disguise he...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           movie_name                            genre  \\\n",
       "0         Silent Hill                  Horror, Mystery   \n",
       "1  Breaking the Waves                   Drama, Romance   \n",
       "2          Wind Chill          Drama, Horror, Thriller   \n",
       "3         Godmothered          Family, Fantasy, Comedy   \n",
       "4         Donkey Skin  Fantasy, Comedy, Music, Romance   \n",
       "\n",
       "                                         description  \n",
       "0  Rose, a desperate mother takes her adopted dau...  \n",
       "1  In a small and conservative Scottish village, ...  \n",
       "2  Two college students share a ride home for the...  \n",
       "3  A young and unskilled fairy godmother that ven...  \n",
       "4  A fairy godmother helps a princess disguise he...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = Path(\"../dataset_train.csv\")\n",
    "test_dir = Path(\"../dataset_test.csv\")\n",
    "\n",
    "df = pd.read_csv(train_dir)\n",
    "print(f\"Dataset size: {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74d9e662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 18\n",
      "Label distribution shape: (8475, 18)\n"
     ]
    }
   ],
   "source": [
    "df[\"text\"] = df[\"movie_name\"].fillna(\"\") + \" [SEP] \" + df[\"description\"].fillna(\"\")\n",
    "y_list = df[\"genre\"].apply(lambda s: [g.strip() for g in str(s).split(\",\") if g.strip()])\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(y_list)\n",
    "\n",
    "print(f\"Number of labels: {len(mlb.classes_)}\")\n",
    "print(f\"Label distribution shape: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6b339d",
   "metadata": {},
   "source": [
    "## 3. Data Augmentation - Back Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "223eb927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_translate(texts, src_lang='en', pivot_lang='fr', sample_ratio=0.2):\n",
    "    model_name_en_pivot = f'Helsinki-NLP/opus-mt-{src_lang}-{pivot_lang}'\n",
    "    model_name_pivot_en = f'Helsinki-NLP/opus-mt-{pivot_lang}-{src_lang}'\n",
    "    \n",
    "    tokenizer_en_pivot = MarianTokenizer.from_pretrained(model_name_en_pivot)\n",
    "    model_en_pivot = MarianMTModel.from_pretrained(model_name_en_pivot)\n",
    "    \n",
    "    tokenizer_pivot_en = MarianTokenizer.from_pretrained(model_name_pivot_en)\n",
    "    model_pivot_en = MarianMTModel.from_pretrained(model_name_pivot_en)\n",
    "    \n",
    "    augmented_texts = []\n",
    "    indices_to_augment = np.random.choice(len(texts), size=int(len(texts) * sample_ratio), replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(indices_to_augment):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Augmenting {i}/{len(indices_to_augment)}...\", end='\\r')\n",
    "        \n",
    "        text = texts.iloc[idx] if hasattr(texts, 'iloc') else texts[idx]\n",
    "        \n",
    "        translated = model_en_pivot.generate(**tokenizer_en_pivot(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128))\n",
    "        pivot_text = tokenizer_en_pivot.decode(translated[0], skip_special_tokens=True)\n",
    "        \n",
    "        back_translated = model_pivot_en.generate(**tokenizer_pivot_en(pivot_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128))\n",
    "        final_text = tokenizer_pivot_en.decode(back_translated[0], skip_special_tokens=True)\n",
    "        \n",
    "        augmented_texts.append(final_text)\n",
    "    \n",
    "    print(f\"Augmentation complete!\" + \" \"*20)\n",
    "    return augmented_texts, indices_to_augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17b7154f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 8475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting 0/1695...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation complete!                    \n",
      "Augmentation complete!                    \n",
      "Augmented dataset size: 10170\n",
      "Training samples: 9153, Validation samples: 1017\n",
      "Augmented dataset size: 10170\n",
      "Training samples: 9153, Validation samples: 1017\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original dataset size: {len(df)}\")\n",
    "\n",
    "augmented_texts, aug_indices = back_translate(df[\"text\"], sample_ratio=0.2)\n",
    "\n",
    "df_augmented = pd.DataFrame({\n",
    "    'movie_name': [df.iloc[i]['movie_name'] for i in aug_indices],\n",
    "    'description': augmented_texts,\n",
    "    'genre': [df.iloc[i]['genre'] for i in aug_indices]\n",
    "})\n",
    "\n",
    "df_combined = pd.concat([df, df_augmented], ignore_index=True)\n",
    "print(f\"Augmented dataset size: {len(df_combined)}\")\n",
    "\n",
    "df_combined[\"text\"] = df_combined[\"movie_name\"].fillna(\"\") + \" [SEP] \" + df_combined[\"description\"].fillna(\"\")\n",
    "y_list_combined = df_combined[\"genre\"].apply(lambda s: [g.strip() for g in str(s).split(\",\") if g.strip()])\n",
    "Y_combined = mlb.fit_transform(y_list_combined)\n",
    "\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(df_combined[\"text\"], Y_combined, test_size=0.1, random_state=42)\n",
    "print(f\"Training samples: {len(X_tr)}, Validation samples: {len(X_va)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2434d4e",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b7b233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined TF-IDF features shape: (9153, 205446)\n"
     ]
    }
   ],
   "source": [
    "tfidf_word = TfidfVectorizer(\n",
    "    ngram_range=(1,3),\n",
    "    min_df=2,\n",
    "    max_features=500_000,\n",
    "    sublinear_tf=True,\n",
    "    stop_words=\"english\",\n",
    "    max_df=0.85,\n",
    "    strip_accents='unicode',\n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "tfidf_char = TfidfVectorizer(\n",
    "    analyzer=\"char_wb\",\n",
    "    ngram_range=(3,6),\n",
    "    min_df=2,\n",
    "    max_features=500_000,\n",
    "    sublinear_tf=True,\n",
    "    max_df=0.85,\n",
    "    strip_accents='unicode'\n",
    ")\n",
    "\n",
    "Xw_tr = tfidf_word.fit_transform(X_tr)\n",
    "Xw_va = tfidf_word.transform(X_va)\n",
    "Xc_tr = tfidf_char.fit_transform(X_tr)\n",
    "Xc_va = tfidf_char.transform(X_va)\n",
    "\n",
    "XTR_tfidf = sp_hstack([Xw_tr, Xc_tr], format=\"csr\")\n",
    "XVA_tfidf = sp_hstack([Xw_va, Xc_va], format=\"csr\")\n",
    "print(f\"Combined TF-IDF features shape: {XTR_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5852442",
   "metadata": {},
   "source": [
    "## 5. Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2969911f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 287/287 [00:53<00:00,  5.32it/s]\n",
      "Batches:   0%|          | 0/32 [00:00<?, ?it/s]\n",
      "Batches: 100%|██████████| 32/32 [00:05<00:00,  5.50it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined features (TF-IDF + Embeddings) shape: (9153, 205830)\n"
     ]
    }
   ],
   "source": [
    "st_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Generating embeddings...\")\n",
    "emb_tr = st_model.encode(X_tr.tolist(), show_progress_bar=True, batch_size=32)\n",
    "emb_va = st_model.encode(X_va.tolist(), show_progress_bar=True, batch_size=32)\n",
    "\n",
    "XTR_combined = sp_hstack([XTR_tfidf, csr_matrix(emb_tr)], format=\"csr\")\n",
    "XVA_combined = sp_hstack([XVA_tfidf, csr_matrix(emb_va)], format=\"csr\")\n",
    "print(f\"Combined features (TF-IDF + Embeddings) shape: {XTR_combined.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aaf274",
   "metadata": {},
   "source": [
    "## 6. Modelos Base - Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89b6db2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LogisticRegression...\n",
      "LogReg training complete!\n",
      "LogReg training complete!\n"
     ]
    }
   ],
   "source": [
    "clf_logreg = OneVsRestClassifier(\n",
    "    LogisticRegression(C=8.0, solver=\"saga\", max_iter=4000, class_weight='balanced', random_state=42),\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(\"Training LogisticRegression...\")\n",
    "clf_logreg.fit(XTR_combined, y_tr)\n",
    "print(\"LogReg training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4045bc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg - micro-F1: 0.7315, macro-F1: 0.6982\n"
     ]
    }
   ],
   "source": [
    "logits_logreg = clf_logreg.decision_function(XVA_combined)\n",
    "ths_logreg = np.zeros(logits_logreg.shape[1])\n",
    "\n",
    "for k in range(logits_logreg.shape[1]):\n",
    "    s = logits_logreg[:, k]\n",
    "    best_f1, best_t = 0.0, 0.0\n",
    "    candidates = np.quantile(s, np.linspace(0.01, 0.99, 50))\n",
    "    for t in candidates:\n",
    "        preds_k = (s >= t).astype(int)\n",
    "        f1 = f1_score(y_va[:, k], preds_k, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, t\n",
    "    ths_logreg[k] = best_t\n",
    "\n",
    "pred_logreg = (logits_logreg >= ths_logreg).astype(int)\n",
    "print(f\"LogReg - micro-F1: {f1_score(y_va, pred_logreg, average='micro'):.4f}, macro-F1: {f1_score(y_va, pred_logreg, average='macro'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a7c0a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost...\n",
      "XGBoost training complete!\n",
      "XGBoost training complete!\n"
     ]
    }
   ],
   "source": [
    "clf_xgb = MultiOutputClassifier(\n",
    "    XGBClassifier(n_estimators=300, max_depth=6, learning_rate=0.1, random_state=42, n_jobs=-1)\n",
    ")\n",
    "print(\"Training XGBoost...\")\n",
    "clf_xgb.fit(emb_tr, y_tr)\n",
    "print(\"XGBoost training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be8ba68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost - micro-F1: 0.6934, macro-F1: 0.6395\n"
     ]
    }
   ],
   "source": [
    "pred_proba_xgb = clf_xgb.predict_proba(emb_va)\n",
    "logits_xgb = np.column_stack([p[:, 1] for p in pred_proba_xgb])\n",
    "ths_xgb = np.zeros(logits_xgb.shape[1])\n",
    "\n",
    "for k in range(logits_xgb.shape[1]):\n",
    "    s = logits_xgb[:, k]\n",
    "    best_f1, best_t = 0.0, 0.0\n",
    "    candidates = np.quantile(s, np.linspace(0.01, 0.99, 50))\n",
    "    for t in candidates:\n",
    "        preds_k = (s >= t).astype(int)\n",
    "        f1 = f1_score(y_va[:, k], preds_k, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, t\n",
    "    ths_xgb[k] = best_t\n",
    "\n",
    "pred_xgb = (logits_xgb >= ths_xgb).astype(int)\n",
    "print(f\"XGBoost - micro-F1: {f1_score(y_va, pred_xgb, average='micro'):.4f}, macro-F1: {f1_score(y_va, pred_xgb, average='macro'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f5c496b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LinearSVC...\n",
      "SVC training complete!\n",
      "SVC training complete!\n"
     ]
    }
   ],
   "source": [
    "clf_svc = OneVsRestClassifier(\n",
    "    LinearSVC(C=2.0, max_iter=4000, class_weight='balanced', dual='auto', random_state=42),\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(\"Training LinearSVC...\")\n",
    "clf_svc.fit(XTR_tfidf, y_tr)\n",
    "print(\"SVC training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a554fc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC - micro-F1: 0.7258, macro-F1: 0.7030\n"
     ]
    }
   ],
   "source": [
    "logits_svc = clf_svc.decision_function(XVA_tfidf)\n",
    "ths_svc = np.zeros(logits_svc.shape[1])\n",
    "\n",
    "for k in range(logits_svc.shape[1]):\n",
    "    s = logits_svc[:, k]\n",
    "    best_f1, best_t = 0.0, 0.0\n",
    "    candidates = np.quantile(s, np.linspace(0.01, 0.99, 50))\n",
    "    for t in candidates:\n",
    "        preds_k = (s >= t).astype(int)\n",
    "        f1 = f1_score(y_va[:, k], preds_k, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, t\n",
    "    ths_svc[k] = best_t\n",
    "\n",
    "pred_svc = (logits_svc >= ths_svc).astype(int)\n",
    "print(f\"LinearSVC - micro-F1: {f1_score(y_va, pred_svc, average='micro'):.4f}, macro-F1: {f1_score(y_va, pred_svc, average='macro'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c495c025",
   "metadata": {},
   "source": [
    "## 7. DistilBERT Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cb91495",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieGenreDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx]) if hasattr(self.texts, 'iloc') else str(self.texts[idx])\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5a3def6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets created: 9153 training, 1017 validation\n"
     ]
    }
   ],
   "source": [
    "tokenizer_distilbert = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model_distilbert = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=len(mlb.classes_),\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "train_dataset_distilbert = MovieGenreDataset(X_tr, y_tr, tokenizer_distilbert, max_length=128)\n",
    "val_dataset_distilbert = MovieGenreDataset(X_va, y_va, tokenizer_distilbert, max_length=128)\n",
    "print(f\"Datasets created: {len(train_dataset_distilbert)} training, {len(val_dataset_distilbert)} validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e94531a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DistilBERT...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1719' max='1719' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1719/1719 59:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.265900</td>\n",
       "      <td>0.261026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.196500</td>\n",
       "      <td>0.209570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.145900</td>\n",
       "      <td>0.203075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT training complete!\n"
     ]
    }
   ],
   "source": [
    "training_args_distilbert = TrainingArguments(\n",
    "    output_dir='./distilbert_results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer_distilbert = Trainer(\n",
    "    model=model_distilbert,\n",
    "    args=training_args_distilbert,\n",
    "    train_dataset=train_dataset_distilbert,\n",
    "    eval_dataset=val_dataset_distilbert,\n",
    ")\n",
    "\n",
    "print(\"Training DistilBERT...\")\n",
    "trainer_distilbert.train()\n",
    "print(\"DistilBERT training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "850d9aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT - micro-F1: 0.7234, macro-F1: 0.6736\n"
     ]
    }
   ],
   "source": [
    "model_distilbert.eval()\n",
    "with torch.no_grad():\n",
    "    val_inputs = tokenizer_distilbert(X_va.tolist(), truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "    outputs = model_distilbert(**val_inputs)\n",
    "    logits_distilbert = torch.sigmoid(outputs.logits).cpu().numpy()\n",
    "\n",
    "ths_distilbert = np.zeros(logits_distilbert.shape[1])\n",
    "for k in range(logits_distilbert.shape[1]):\n",
    "    s = logits_distilbert[:, k]\n",
    "    best_f1, best_t = 0.0, 0.0\n",
    "    candidates = np.quantile(s, np.linspace(0.01, 0.99, 50))\n",
    "    for t in candidates:\n",
    "        preds_k = (s >= t).astype(int)\n",
    "        f1 = f1_score(y_va[:, k], preds_k, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, t\n",
    "    ths_distilbert[k] = best_t\n",
    "\n",
    "pred_distilbert = (logits_distilbert >= ths_distilbert).astype(int)\n",
    "print(f\"DistilBERT - micro-F1: {f1_score(y_va, pred_distilbert, average='micro'):.4f}, macro-F1: {f1_score(y_va, pred_distilbert, average='macro'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e182905",
   "metadata": {},
   "source": [
    "## 8. DeBERTa-v3 (Top Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53240274",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeBERTa datasets created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "tokenizer_deberta = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "model_deberta = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"microsoft/deberta-v3-base\",\n",
    "    num_labels=len(mlb.classes_),\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "train_dataset_deberta = MovieGenreDataset(X_tr, y_tr, tokenizer_deberta, max_length=256)\n",
    "val_dataset_deberta = MovieGenreDataset(X_va, y_va, tokenizer_deberta, max_length=256)\n",
    "print(f\"DeBERTa datasets created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b37144ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DeBERTa-v3...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='694' max='2865' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 694/2865 2:14:03 < 7:00:34, 0.09 it/s, Epoch 1.21/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.279900</td>\n",
       "      <td>0.265815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     17\u001b[39m trainer_deberta = Trainer(\n\u001b[32m     18\u001b[39m     model=model_deberta,\n\u001b[32m     19\u001b[39m     args=training_args_deberta,\n\u001b[32m     20\u001b[39m     train_dataset=train_dataset_deberta,\n\u001b[32m     21\u001b[39m     eval_dataset=val_dataset_deberta,\n\u001b[32m     22\u001b[39m )\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining DeBERTa-v3...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mtrainer_deberta\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDeBERTa training complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shiyi Cheng yi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shiyi Cheng yi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\trainer.py:2715\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2713\u001b[39m         grad_norm_context = implicit_replication\n\u001b[32m   2714\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m grad_norm_context():\n\u001b[32m-> \u001b[39m\u001b[32m2715\u001b[39m         _grad_norm = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2716\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2717\u001b[39m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2718\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2720\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2721\u001b[39m     is_accelerate_available()\n\u001b[32m   2722\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED\n\u001b[32m   2723\u001b[39m ):\n\u001b[32m   2724\u001b[39m     grad_norm = model.get_global_grad_norm()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shiyi Cheng yi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\accelerate\\accelerator.py:2897\u001b[39m, in \u001b[36mAccelerator.clip_grad_norm_\u001b[39m\u001b[34m(self, parameters, max_norm, norm_type)\u001b[39m\n\u001b[32m   2895\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m model.clip_grad_norm_(max_norm, norm_type)\n\u001b[32m   2896\u001b[39m \u001b[38;5;28mself\u001b[39m.unscale_gradients()\n\u001b[32m-> \u001b[39m\u001b[32m2897\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shiyi Cheng yi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:43\u001b[39m, in \u001b[36m_no_grad.<locals>._no_grad_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_no_grad_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shiyi Cheng yi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:232\u001b[39m, in \u001b[36mclip_grad_norm_\u001b[39m\u001b[34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[39m\n\u001b[32m    230\u001b[39m grads = [p.grad \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters \u001b[38;5;28;01mif\u001b[39;00m p.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    231\u001b[39m total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m \u001b[43m_clip_grads_with_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total_norm\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shiyi Cheng yi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:43\u001b[39m, in \u001b[36m_no_grad.<locals>._no_grad_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_no_grad_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shiyi Cheng yi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:174\u001b[39m, in \u001b[36m_clip_grads_with_norm_\u001b[39m\u001b[34m(parameters, max_norm, total_norm, foreach)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (device, _), ([device_grads], _) \u001b[38;5;129;01min\u001b[39;00m grouped_grads.items():\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (foreach \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m _has_foreach_support(device_grads, device)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m    172\u001b[39m         foreach \u001b[38;5;129;01mand\u001b[39;00m _device_has_foreach_support(device)\n\u001b[32m    173\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m         \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_foreach_mul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_coef_clamped\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m foreach:\n\u001b[32m    176\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    177\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mforeach=True was passed, but can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt use the foreach API on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice.type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tensors\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    178\u001b[39m         )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "training_args_deberta = TrainingArguments(\n",
    "    output_dir='./deberta_results',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=2,\n",
    ")\n",
    "\n",
    "trainer_deberta = Trainer(\n",
    "    model=model_deberta,\n",
    "    args=training_args_deberta,\n",
    "    train_dataset=train_dataset_deberta,\n",
    "    eval_dataset=val_dataset_deberta,\n",
    ")\n",
    "\n",
    "print(\"Training DeBERTa-v3...\")\n",
    "trainer_deberta.train()\n",
    "print(\"DeBERTa training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fa464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deberta.eval()\n",
    "with torch.no_grad():\n",
    "    val_inputs = tokenizer_deberta(X_va.tolist(), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "    outputs = model_deberta(**val_inputs)\n",
    "    logits_deberta = torch.sigmoid(outputs.logits).cpu().numpy()\n",
    "\n",
    "ths_deberta = np.zeros(logits_deberta.shape[1])\n",
    "for k in range(logits_deberta.shape[1]):\n",
    "    s = logits_deberta[:, k]\n",
    "    best_f1, best_t = 0.0, 0.0\n",
    "    candidates = np.quantile(s, np.linspace(0.01, 0.99, 50))\n",
    "    for t in candidates:\n",
    "        preds_k = (s >= t).astype(int)\n",
    "        f1 = f1_score(y_va[:, k], preds_k, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, t\n",
    "    ths_deberta[k] = best_t\n",
    "\n",
    "pred_deberta = (logits_deberta >= ths_deberta).astype(int)\n",
    "print(f\"DeBERTa-v3 - micro-F1: {f1_score(y_va, pred_deberta, average='micro'):.4f}, macro-F1: {f1_score(y_va, pred_deberta, average='macro'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f1c66e",
   "metadata": {},
   "source": [
    "## 9. Ensemble Optimization con Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e523e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    w_deberta = trial.suggest_float(\"w_deberta\", 0.3, 0.6)\n",
    "    w_distilbert = trial.suggest_float(\"w_distilbert\", 0.1, 0.4)\n",
    "    w_logreg = trial.suggest_float(\"w_logreg\", 0.1, 0.3)\n",
    "    w_xgb = trial.suggest_float(\"w_xgb\", 0.05, 0.25)\n",
    "    w_svc = max(0.0, 1.0 - w_deberta - w_distilbert - w_logreg - w_xgb)\n",
    "    \n",
    "    ensemble_logits_opt = (w_deberta * logits_deberta + \n",
    "                           w_distilbert * logits_distilbert + \n",
    "                           w_logreg * logits_logreg + \n",
    "                           w_xgb * logits_xgb + \n",
    "                           w_svc * logits_svc)\n",
    "    \n",
    "    ths_opt = np.zeros(ensemble_logits_opt.shape[1])\n",
    "    for k in range(ensemble_logits_opt.shape[1]):\n",
    "        s = ensemble_logits_opt[:, k]\n",
    "        best_f1, best_t = 0.0, 0.0\n",
    "        candidates = np.quantile(s, np.linspace(0.01, 0.99, 30))\n",
    "        for t in candidates:\n",
    "            preds_k = (s >= t).astype(int)\n",
    "            f1 = f1_score(y_va[:, k], preds_k, zero_division=0)\n",
    "            if f1 > best_f1:\n",
    "                best_f1, best_t = f1, t\n",
    "        ths_opt[k] = best_t\n",
    "    \n",
    "    pred_opt = (ensemble_logits_opt >= ths_opt).astype(int)\n",
    "    return f1_score(y_va, pred_opt, average='macro')\n",
    "\n",
    "print(\"Optimizing ensemble weights with Optuna...\")\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(f\"\\nBest F1 macro: {study.best_value:.4f}\")\n",
    "print(\"Best weights:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da5fd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "w_deberta_opt = best_params['w_deberta']\n",
    "w_distilbert_opt = best_params['w_distilbert']\n",
    "w_logreg_opt = best_params['w_logreg']\n",
    "w_xgb_opt = best_params['w_xgb']\n",
    "w_svc_opt = 1.0 - w_deberta_opt - w_distilbert_opt - w_logreg_opt - w_xgb_opt\n",
    "\n",
    "ensemble_optimized = (w_deberta_opt * logits_deberta + \n",
    "                      w_distilbert_opt * logits_distilbert + \n",
    "                      w_logreg_opt * logits_logreg + \n",
    "                      w_xgb_opt * logits_xgb + \n",
    "                      w_svc_opt * logits_svc)\n",
    "\n",
    "ths_optimized = np.zeros(ensemble_optimized.shape[1])\n",
    "for k in range(ensemble_optimized.shape[1]):\n",
    "    s = ensemble_optimized[:, k]\n",
    "    best_f1, best_t = 0.0, 0.0\n",
    "    candidates = np.quantile(s, np.linspace(0.01, 0.99, 50))\n",
    "    for t in candidates:\n",
    "        preds_k = (s >= t).astype(int)\n",
    "        f1 = f1_score(y_va[:, k], preds_k, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, t\n",
    "    ths_optimized[k] = best_t\n",
    "\n",
    "pred_optimized = (ensemble_optimized >= ths_optimized).astype(int)\n",
    "print(f\"Optimized Ensemble - micro-F1: {f1_score(y_va, pred_optimized, average='micro'):.4f}, macro-F1: {f1_score(y_va, pred_optimized, average='macro'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6c5181",
   "metadata": {},
   "source": [
    "## 10. Test Time Augmentation y Predicción Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdc72c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tta_predict_deberta(texts, model, tokenizer, n_augmentations=3):\n",
    "    all_predictions = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_inputs = tokenizer(texts, truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "        outputs = model(**test_inputs)\n",
    "        all_predictions.append(torch.sigmoid(outputs.logits).cpu().numpy())\n",
    "    \n",
    "    for _ in range(n_augmentations):\n",
    "        model.train()\n",
    "        with torch.no_grad():\n",
    "            test_inputs = tokenizer(texts, truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "            outputs = model(**test_inputs)\n",
    "            all_predictions.append(torch.sigmoid(outputs.logits).cpu().numpy())\n",
    "    \n",
    "    return np.mean(all_predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28266286",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(test_dir)\n",
    "df_test[\"text\"] = df_test[\"movie_name\"].fillna(\"\") + \" [SEP] \" + df_test[\"description\"].fillna(\"\")\n",
    "\n",
    "print(\"Generating DeBERTa predictions with TTA...\")\n",
    "logits_deberta_test_tta = tta_predict_deberta(df_test[\"text\"].tolist(), model_deberta, tokenizer_deberta)\n",
    "\n",
    "print(\"Generating other model predictions...\")\n",
    "Xw_test = tfidf_word.transform(df_test[\"text\"])\n",
    "Xc_test = tfidf_char.transform(df_test[\"text\"])\n",
    "X_test_tfidf = sp_hstack([Xw_test, Xc_test], format=\"csr\")\n",
    "emb_test = st_model.encode(df_test[\"text\"].tolist(), show_progress_bar=True, batch_size=32)\n",
    "X_test_combined = sp_hstack([X_test_tfidf, csr_matrix(emb_test)], format=\"csr\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_inputs = tokenizer_distilbert(df_test[\"text\"].tolist(), truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "    outputs = model_distilbert(**test_inputs)\n",
    "    logits_distilbert_test = torch.sigmoid(outputs.logits).cpu().numpy()\n",
    "\n",
    "logits_logreg_test = clf_logreg.decision_function(X_test_combined)\n",
    "pred_proba_xgb_test = clf_xgb.predict_proba(emb_test)\n",
    "logits_xgb_test = np.column_stack([p[:, 1] for p in pred_proba_xgb_test])\n",
    "logits_svc_test = clf_svc.decision_function(X_test_tfidf)\n",
    "\n",
    "print(\"Creating optimized ensemble...\")\n",
    "ensemble_final_test = (w_deberta_opt * logits_deberta_test_tta + \n",
    "                       w_distilbert_opt * logits_distilbert_test + \n",
    "                       w_logreg_opt * logits_logreg_test + \n",
    "                       w_xgb_opt * logits_xgb_test + \n",
    "                       w_svc_opt * logits_svc_test)\n",
    "\n",
    "pred_test_final = (ensemble_final_test >= ths_optimized).astype(int)\n",
    "\n",
    "pred_labels = [\", \".join([mlb.classes_[j] for j, v in enumerate(row) if v == 1]) for row in pred_test_final]\n",
    "result_df = pd.DataFrame({\n",
    "    \"movie_name\": df_test[\"movie_name\"],\n",
    "    \"genre\": pred_labels,\n",
    "    \"description\": df_test[\"description\"]\n",
    "})\n",
    "result_df.to_csv(\"dataset_test_preds_optimized.csv\", index=False)\n",
    "print(f\"Optimized predictions saved: {len(result_df)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749e1a38",
   "metadata": {},
   "source": [
    "## 11. Resumen de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2a296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FINAL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"1. LogReg (TF-IDF+Embed):       micro-F1: {f1_score(y_va, pred_logreg, average='micro'):.4f}, macro-F1: {f1_score(y_va, pred_logreg, average='macro'):.4f}\")\n",
    "print(f\"2. XGBoost (Embeddings):        micro-F1: {f1_score(y_va, pred_xgb, average='micro'):.4f}, macro-F1: {f1_score(y_va, pred_xgb, average='macro'):.4f}\")\n",
    "print(f\"3. LinearSVC (TF-IDF):          micro-F1: {f1_score(y_va, pred_svc, average='micro'):.4f}, macro-F1: {f1_score(y_va, pred_svc, average='macro'):.4f}\")\n",
    "print(f\"4. DistilBERT (Fine-tuned):     micro-F1: {f1_score(y_va, pred_distilbert, average='micro'):.4f}, macro-F1: {f1_score(y_va, pred_distilbert, average='macro'):.4f}\")\n",
    "print(f\"5. DeBERTa-v3 (Fine-tuned):     micro-F1: {f1_score(y_va, pred_deberta, average='micro'):.4f}, macro-F1: {f1_score(y_va, pred_deberta, average='macro'):.4f}\")\n",
    "print(f\"6. OPTIMIZED ENSEMBLE (All 5):  micro-F1: {f1_score(y_va, pred_optimized, average='micro'):.4f}, macro-F1: {f1_score(y_va, pred_optimized, average='macro'):.4f}\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
